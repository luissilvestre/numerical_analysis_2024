{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeed4ec0",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "Solve exercises 8.6, 9.7, 9.8, 9.9 and 9.10.\n",
    "\n",
    "We start by copying all the functions from the last homework. We will use them for comparison with the new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function LU(A)\n",
    "    n, m = size(A) # A is supposed to be a square matrix, so hopefully n and m will be equal.\n",
    "    \n",
    "    # We initalize L with zeros and U to be the same as A.\n",
    "    L = zeros(n,m)\n",
    "    U = copy(A)\n",
    "    \n",
    "    for k in 1:n\n",
    "        L[k,k] = 1\n",
    "        for i in (k+1):n\n",
    "            L[i,k] = U[i,k]/U[k,k]\n",
    "            U[i,:] = U[i,:] - L[i,k]*U[k,:]\n",
    "        end\n",
    "    end\n",
    "    return L, U\n",
    "end\n",
    "\n",
    "function backward_substitute(U,y)\n",
    "    n, m = size(U)\n",
    "    r, = size(y)\n",
    "    @assert n == m == r\n",
    "    x = zeros(n)\n",
    "    \n",
    "    for i in n:-1:1\n",
    "        tail = 0\n",
    "        for j in i+1:n\n",
    "            tail = tail + U[i,j]*x[j]\n",
    "        end\n",
    "        x[i] = (y[i] - tail)/U[i,i]\n",
    "    end\n",
    "    \n",
    "    return x\n",
    "end\n",
    "\n",
    "function forward_substitute(L,y)\n",
    "    n, m = size(L)\n",
    "    r, = size(y)\n",
    "    @assert n == m == r\n",
    "    x = zeros(n)\n",
    "    for i in 1:n\n",
    "        tail = 0\n",
    "        for j in 1:i-1\n",
    "            tail = tail + L[i,j]*x[j]\n",
    "        end\n",
    "        x[i] = (y[i] - tail)/L[i,i]\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "function solve(A,f)\n",
    "    n, m = size(A)\n",
    "    r, = size(f)\n",
    "    @assert n == m == r\n",
    "    \n",
    "    # Something goes here\n",
    "    L, U = LU(A)\n",
    "    y = forward_substitute(L,f)\n",
    "    x = backward_substitute(U,y)\n",
    "    \n",
    "    return(x)\n",
    "end\n",
    "\n",
    "function big_matrix(n)\n",
    "    A = zeros(n^2,n^2)\n",
    "    for i in 1:n^2\n",
    "        A[i,i] = 4\n",
    "        if mod(i,n)!=0\n",
    "            A[i+1,i] = -1\n",
    "            A[i,i+1] = -1\n",
    "        end\n",
    "        if i<=n^2-n\n",
    "            A[i,i+n] = -1\n",
    "            A[i+n,i] = -1\n",
    "        end\n",
    "    end\n",
    "    return A\n",
    "end\n",
    "\n",
    "function big_rhs(n, f)\n",
    "    y = zeros(n^2)\n",
    "    for i in 1:n\n",
    "        y[i] += f(i/(n+1),0.)\n",
    "        y[n^2-n+i] += f(i/(n+1),1.)\n",
    "    end\n",
    "        \n",
    "    for i in 1:n\n",
    "        y[n*i] += f(1.,i/(n+1))\n",
    "        y[n*i-n+1] += f(0.,i/(n+1))\n",
    "    end\n",
    "    return y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887b604",
   "metadata": {},
   "source": [
    "We are going to solve the elastic membrane problem with various methods and compare the results. Recall that the problem consists in looking for values $u_{i,j}$ in some mesh so that\n",
    "$$ \\begin{cases} \n",
    "u_{i,j} = f\\left( \\frac i {n+1},\\frac j {n+1} \\right) & \\text{if either $i$ or $j$ is equal to either $0$ or $N+1$}, \\\\\n",
    "4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = 0 & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "It is a system of $n \\times n$ linear equations. In order to apply Gaussian Elimination, we wrote it as an $n^2 \\times n^2$ matrix (provided by the function `big_matrix(n)`) that applies to the vector $(u_{1,1},\\dots, u_{1,n},u_{2,1},u_{2,2},\\dots,u_{n,n})$.\n",
    "\n",
    "I am not too happy about writing the elements of a 2-dimensional mesh of points as a one-dimensional vector in $\\mathbb R^{n^2}$. How about we better search for a vector $u$ that is an $n \\times n$ matrix. We define the linear transformation $T : \\mathbb R^{n \\times n} \\to \\mathbb R^{n \\times n}$ given by\n",
    "$$ T(\\{u_{i,j}\\}) = 4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1},$$\n",
    "with the caveat that we do not subtract any term that falls outside of the $1..n \\times 1..n$ mesh. Thus, the system becomes\n",
    "$$ Tu = b, $$\n",
    "where \n",
    "$$ b_{i,j} = \\begin{cases}\n",
    "f\\left( 0,\\frac i {n+1} \\right) & \\text{ if } j=1, \\\\\n",
    "f\\left( 1,\\frac i {n+1} \\right) & \\text{ if } j=n, \\\\\n",
    "f\\left( \\frac j {n+1}, 0 \\right) & \\text{ if } i=1, \\\\\n",
    "f\\left( \\frac j {n+1}, 1 \\right) & \\text{ if } i=n, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases} $$\n",
    "\n",
    "The linear operator $T$ is easy to write as a function. Also the right hand side $b$. Here they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bfe479",
   "metadata": {},
   "outputs": [],
   "source": [
    "function T(u::Array{<:Real,2})\n",
    "    n, m = size(u)\n",
    "    v = similar(u) # This line creates another array with the same dimensions and type as u\n",
    "    for i in 1:n\n",
    "        for j in 1:m\n",
    "            v[i,j] = 4*u[i,j]\n",
    "            if i>1 v[i,j] -= u[i-1,j] end\n",
    "            if i<n v[i,j] -= u[i+1,j] end\n",
    "            if j>1 v[i,j] -= u[i,j-1] end\n",
    "            if j<m v[i,j] -= u[i,j+1] end\n",
    "        end\n",
    "    end\n",
    "    return v\n",
    "end\n",
    "\n",
    "function matrix_rhs(n::Integer, f::Function)\n",
    "    b = zeros(n,n)\n",
    "    for i in 1:n\n",
    "        b[1,i] += f(0.,i/(n+1))\n",
    "        b[n,i] += f(1.,i/(n+1))\n",
    "        b[i,1] += f(i/(n+1),0.)\n",
    "        b[i,n] += f(i/(n+1),1.)\n",
    "    end\n",
    "    return b\n",
    "end        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab0861",
   "metadata": {},
   "source": [
    "A convenient feature about iterative methods is that we do not need to have the matrix stored anywhere. We may solve the system with the required linear transformations written as functions.\n",
    "\n",
    "Let us try Jacobi iteration first. We need two functions. One of them is the inverse of the diagonal, and the other is everything off-diagonal. They would be the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_diagonal(u) = 0.25 * u\n",
    "\n",
    "function off_diagonal(u::Array{<:Real,2})\n",
    "    n, m = size(u)\n",
    "    v = zeros(n,m)\n",
    "    for i in 1:n\n",
    "        for j in 1:m\n",
    "            if i>1 v[i,j] -= u[i-1,j] end\n",
    "            if i<n v[i,j] -= u[i+1,j] end\n",
    "            if j>1 v[i,j] -= u[i,j-1] end\n",
    "            if j<m v[i,j] -= u[i,j+1] end\n",
    "        end\n",
    "    end\n",
    "    return v\n",
    "end\n",
    "\n",
    "function jacobi(u::Array{<:Real,2}, b::Array{<:Real,2}, iterations::Integer)\n",
    "    for i in 1:iterations\n",
    "        u = inverse_diagonal(b - off_diagonal(u))\n",
    "    end\n",
    "    return u\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da10d8",
   "metadata": {},
   "source": [
    "Let us try it and compare with Gaussian elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1cc11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "n = 20\n",
    "f(x,y) = 1 - sin(pi*x)\n",
    "partition = 1/(n+1) : 1/(n+1) : 1-1/(n+1)\n",
    "\n",
    "# We use the following function to roughly measure the size of vectors or matrices.\n",
    "function supnorm(u)\n",
    "    s = 0\n",
    "    for entry in u\n",
    "        s = max(abs(entry),s)\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "# Using Gaussian elimination\n",
    "y = big_rhs(n,f)\n",
    "A = big_matrix(n)\n",
    "x = solve(A,y)\n",
    "membrane = reshape(x,n,n)\n",
    "sg = surface(partition,partition,membrane)\n",
    "println(\"Accuracy of Gaussian elimination: \", supnorm(A*x-y) / n^2)\n",
    "\n",
    "# Using Jacobi iteration\n",
    "# We can test different number of iterations\n",
    "iterations = 500\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n) # We take an initial guess that is identically zero.\n",
    "u = jacobi(u,b,iterations)\n",
    "println(\"Accuracy of Jacobi with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "sj = surface(partition,partition,u)\n",
    "display(sg)\n",
    "display(sj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a6b96b",
   "metadata": {},
   "source": [
    "It would take more iterations for the Jacobi method to give us a solution as accurate as Gaussian elimination. But is that a bad thing? Maybe we do not want such an accurate solution. Iterative methods give us a choice of speed vs accuracy. We do not have that choice in all-or-nothin Gaussian elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266864d4",
   "metadata": {},
   "source": [
    "Let us implement Gauss-Seidel now. **You have to write the missing part of the function below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, we do not want to use the big_matrix.\n",
    "# We implement the iteration of the Gauss-Seidel method as a function. \n",
    "# If you write it efficiently, it should modify the values of u in place, without the need of using more memory\n",
    "# for intermediate computations.\n",
    "\n",
    "function gauss_seidel_iteration!(u::Array{<:Real,2}, b::Array{<:Real,2})\n",
    "    n, m = size(u)\n",
    "    for i in 1:n\n",
    "        for j in 1:m\n",
    "            # ...\n",
    "            # Something goes here to compute the new value for u[i,j]\n",
    "            # ...\n",
    "        end\n",
    "    end\n",
    "end    \n",
    "\n",
    "function gauss_seidel(u::Array{<:Real,2}, b::Array{<:Real,2}, iterations::Integer)\n",
    "    for i in 1:iterations\n",
    "        gauss_seidel_iteration!(u,b)\n",
    "    end\n",
    "    return u\n",
    "end    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667eba48",
   "metadata": {},
   "source": [
    "Let us test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gauss-Seidel iteration\n",
    "# We can test different number of iterations\n",
    "iterations = 500\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n) # We take an initial guess that is identically zero.\n",
    "u = gauss_seidel(u,b,iterations)\n",
    "println(\"Accuracy of Gauss Seidel with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "sgs = surface(partition,partition,u)\n",
    "display(sgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a933e9",
   "metadata": {},
   "source": [
    "With 500 iterations, I got a more accurate answer with Gauss-Seidel than with Jacobi iteration, but not as accurate as with Gaussian elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c5bed",
   "metadata": {},
   "source": [
    "Let us implement the gradient descent method now. For that, we should only need the functions `T` defined above (that we have not even used yet) and `matrix_rhs`. For your convenience, I provide an function computing the dot product of two \"vectors\" that are expressed as 2-dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "function dot_product(u::Array{<:Real,2}, v::Array{<:Real,2})\n",
    "    return sum(u.*v)\n",
    "end\n",
    "\n",
    "function gradient_descent(u::Array{<:Real,2}, b::Array{<:Real,2}, iterations::Integer)\n",
    "    for i in 1:iterations\n",
    "        # r = ...\n",
    "        # alpha = ...\n",
    "        # u = ...\n",
    "    end\n",
    "    return u\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c19d6e",
   "metadata": {},
   "source": [
    "Time to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ead44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Gradient descent\n",
    "# We can test different number of iterations\n",
    "iterations = 500\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n) # We take an initial guess that is identically zero.\n",
    "u = gradient_descent(u,b,iterations)\n",
    "println(\"Accuracy of Gradient descent with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "sgd = surface(partition,partition,u)\n",
    "display(sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91966b34",
   "metadata": {},
   "source": [
    "The accuracy I got for gradient descent in this case is similar as for Jacobi iteration. \n",
    "\n",
    "Let us test how long these algorithms take in practice. So, we will time their execution time with a 40x40 mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4681ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "f(x,y) = 1 - sin(pi*x)\n",
    "partition = 1/(n+1) : 1/(n+1) : 1-1/(n+1)\n",
    "\n",
    "# Using Gaussian elimination\n",
    "y = big_rhs(n,f)\n",
    "A = big_matrix(n)\n",
    "@time x = solve(A,y)\n",
    "println(\"Accuracy of Gaussian elimination: \", supnorm(A*x-y) / n^2)\n",
    "println()\n",
    "\n",
    "# Using Jacobi iteration\n",
    "iterations = 500\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = jacobi(u,b,iterations)\n",
    "println(\"Accuracy of Jacobi with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "println()\n",
    "\n",
    "# Using Jacobi iteration with more iterations\n",
    "iterations = 5000\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = jacobi(u,b,iterations)\n",
    "println(\"Accuracy of Jacobi with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "println()\n",
    "\n",
    "# Using Gauss-Seidel iteration\n",
    "# We can test different number of iterations\n",
    "iterations = 500\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = gauss_seidel(u,b,iterations)\n",
    "println(\"Accuracy of Gauss Seidel with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "println()\n",
    "\n",
    "# Using Gauss-Seidel iteration\n",
    "# We can test different number of iterations\n",
    "iterations = 5000\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = gauss_seidel(u,b,iterations)\n",
    "println(\"Accuracy of Gauss Seidel with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "println()\n",
    "\n",
    "# Using Gradient descent\n",
    "# We can test different number of iterations\n",
    "iterations = 500\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = gradient_descent(u,b,iterations)\n",
    "println(\"Accuracy of Gradient descent with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "println()\n",
    "\n",
    "# Using Gradient descent\n",
    "# We can test different number of iterations\n",
    "iterations = 5000\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = gradient_descent(u,b,iterations)\n",
    "println(\"Accuracy of Gradient descent with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)\n",
    "println()\n",
    "\n",
    "# Using Gradient descent\n",
    "# We can test different number of iterations\n",
    "iterations = 10000\n",
    "b = matrix_rhs(n,f)\n",
    "u = zeros(n,n)\n",
    "@time u = gradient_descent(u,b,iterations)\n",
    "println(\"Accuracy of Gradient descent with \", iterations, \" iterations: \", supnorm(T(u)-b)/n^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35d1ea",
   "metadata": {},
   "source": [
    "## Wait a minute!!\n",
    "\n",
    "Gaussian elimination takes a really long time!! In my office computer (which is pretty old), Gaussian elimination takes about half a minute. Any of the iterative methods produces an apparently decent approximation of the solution after 500 iterations in less than 1/100 of a second. If we want a solution that is as accurate as Gaussian elimination, we achieve it with more iterations in something like 1/10 of a second. The difference would be even more noticeable with larger values of $n$.\n",
    "\n",
    "Gauss-Seidel seems to converge a bit faster than either Jacobi or Gradient descent. But the difference is not very substatial. It would be fully compensated just by doubling the number of iterations in any of the slower iterative methods.\n",
    "\n",
    "Recall than Gaussian elimination produces an exact solution in theory. The error we see here is the result of floating point truncations in all the intermediate computations.\n",
    "\n",
    "What do you think is the feature of this particular equation that makes the iterative methods so much faster than Gaussian elimination?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393edf13",
   "metadata": {},
   "source": [
    "## Final fun movie\n",
    "\n",
    "We finish in style with an animated picture that shows how the value of our approximation $u$ starts as constant zero and moves toward the actual solution through the iteration of the gradient descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "n = 40\n",
    "partition = 1/(n+1) : 1/(n+1) : 1-1/(n+1)\n",
    "\n",
    "let b = matrix_rhs(n,f), u = zeros(n,n)\n",
    "    @gif for i in 1:300\n",
    "        u = gradient_descent(u,b,iterations)\n",
    "        sgd = surface(partition,partition,u, title=string(i*iterations)*\" iterations\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c1581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
